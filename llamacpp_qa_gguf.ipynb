{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crZcXvqjqiFi"
      },
      "source": [
        "# LLaMA.CPP-QA\n",
        "\n",
        "This project focuses on document-based question answering using llama.cpp. Given a collection of documents, in PDF or text format, the model aims to accurately answer questions and return the sources of the answers, using the tools provided by Langchain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hAqzrS9qiFl"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tni4tmSrqiFm"
      },
      "source": [
        "Install the required Python packages.\n",
        "\n",
        "**Note**: A runtime restart is required after the cell is executed for the changes to be applied, due to conflicts in the versions of numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtZenxJRVDF8"
      },
      "outputs": [],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade llama-cpp-python\n",
        "!pip install --upgrade transformers langchain chromadb sentence_transformers huggingface_hub PyMuPDF numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_1oLeckqiFq"
      },
      "source": [
        "Obtain the quantized model from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td2nzWytGYM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e9af59-7823-4428-dda6-a516fd386ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-08 12:47:03--  https://huggingface.co/TheBloke/llama2-22B-daydreamer-v2-GGUF/resolve/main/llama2-22b-daydreamer-v2.Q4_K_S.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.55, 18.164.174.17, 18.164.174.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/31/39/31392ee792756a71b50c02fe9eb595950af39a9ef700f445df3cc4bb067713f7/cd17bac0940b23b0204725afe39e2794c621b65ed8431734d612f53e16e5a5b6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama2-22b-daydreamer-v2.Q4_K_S.gguf%3B+filename%3D%22llama2-22b-daydreamer-v2.Q4_K_S.gguf%22%3B&Expires=1694436423&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NDQzNjQyM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMS8zOS8zMTM5MmVlNzkyNzU2YTcxYjUwYzAyZmU5ZWI1OTU5NTBhZjM5YTllZjcwMGY0NDVkZjNjYzRiYjA2NzcxM2Y3L2NkMTdiYWMwOTQwYjIzYjAyMDQ3MjVhZmUzOWUyNzk0YzYyMWI2NWVkODQzMTczNGQ2MTJmNTNlMTZlNWE1YjY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=hd0l8pGRVC4j1qQ5AupC6C6ub5nDCCvpaWliobB7oV5-lWhO3TTr9VktwXk00yQquGpIKN3X5KuID45xCRvdM23NlUoIKXp7G0m9NbSlV4osHumLxH4GKBEXnThopeGtmh2onVgJxS9X9IMUdCaMyO4boHp0Ko8-N5rAa%7ETmR6AOSU8St1bN-jo-oWoTga5bH8z%7EB3RBYDCaYnt41TGj7lyit0hOvVOHYx0-XqHGQyOiEOoB6nCo9Q1FFSyKenj3NyBcfRscFaxNZpzt%7EACcbxSJk1An3-58n1qUS7RTz6dqlKatpOOXcvkzSbGuhOB98fnut6%7EtaGc74kJuXzPqzQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-09-08 12:47:03--  https://cdn-lfs.huggingface.co/repos/31/39/31392ee792756a71b50c02fe9eb595950af39a9ef700f445df3cc4bb067713f7/cd17bac0940b23b0204725afe39e2794c621b65ed8431734d612f53e16e5a5b6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama2-22b-daydreamer-v2.Q4_K_S.gguf%3B+filename%3D%22llama2-22b-daydreamer-v2.Q4_K_S.gguf%22%3B&Expires=1694436423&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NDQzNjQyM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMS8zOS8zMTM5MmVlNzkyNzU2YTcxYjUwYzAyZmU5ZWI1OTU5NTBhZjM5YTllZjcwMGY0NDVkZjNjYzRiYjA2NzcxM2Y3L2NkMTdiYWMwOTQwYjIzYjAyMDQ3MjVhZmUzOWUyNzk0YzYyMWI2NWVkODQzMTczNGQ2MTJmNTNlMTZlNWE1YjY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=hd0l8pGRVC4j1qQ5AupC6C6ub5nDCCvpaWliobB7oV5-lWhO3TTr9VktwXk00yQquGpIKN3X5KuID45xCRvdM23NlUoIKXp7G0m9NbSlV4osHumLxH4GKBEXnThopeGtmh2onVgJxS9X9IMUdCaMyO4boHp0Ko8-N5rAa%7ETmR6AOSU8St1bN-jo-oWoTga5bH8z%7EB3RBYDCaYnt41TGj7lyit0hOvVOHYx0-XqHGQyOiEOoB6nCo9Q1FFSyKenj3NyBcfRscFaxNZpzt%7EACcbxSJk1An3-58n1qUS7RTz6dqlKatpOOXcvkzSbGuhOB98fnut6%7EtaGc74kJuXzPqzQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.25.122, 18.65.25.40, 18.65.25.124, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.25.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12417526688 (12G) [binary/octet-stream]\n",
            "Saving to: ‘models/llama2-22b-daydreamer-v2.Q4_K_S.gguf’\n",
            "\n",
            "llama2-22b-daydream 100%[===================>]  11.56G  48.7MB/s    in 4m 17s  \n",
            "\n",
            "2023-09-08 12:51:20 (46.2 MB/s) - ‘models/llama2-22b-daydreamer-v2.Q4_K_S.gguf’ saved [12417526688/12417526688]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir models\n",
        "\n",
        "## The following are the models that have been tested and produced good results.\n",
        "\n",
        "# # PuddleJumper 13B (detailed answers)\n",
        "# !wget -P models/ https://huggingface.co/TheBloke/PuddleJumper-13B-GGUF/resolve/main/puddlejumper-13b.Q6_K.gguf\n",
        "\n",
        "# # llama-2-13B-chat-limarp-v2-merged-GGUF (shorter answers)\n",
        "# !wget -P models/ https://huggingface.co/TheBloke/llama-2-13B-chat-limarp-v2-merged-GGUF/resolve/main/llama-2-13b-chat-limarp-v2-merged.Q6_K.gguf\n",
        "\n",
        "# # MythoMax-L2-Kimiko-v2-13B-GGUF (shorter answers - similar to llama-2-13B-chat-limarp-v2-merged-GGUF)\n",
        "# !wget -P models/ https://huggingface.co/TheBloke/MythoMax-L2-Kimiko-v2-13B-GGUF/resolve/main/mythomax-l2-kimiko-v2-13b.Q6_K.gguf\n",
        "\n",
        "# # Athena-v1-GGUF (shorter answers, but more detailed than previous two - details not always correct)\n",
        "# !wget -P models/ https://huggingface.co/TheBloke/Athena-v1-GGUF/resolve/main/athena-v1.Q6_K.gguf\n",
        "\n",
        "# # TheBloke/Camel-Platypus2-13B-GGUF (similar to PuddleJumper - accurate numbers (maybe))\n",
        "# !wget -P models/ https://huggingface.co/TheBloke/Camel-Platypus2-13B-GGUF/resolve/main/camel-platypus2-13b.Q6_K.gguf\n",
        "\n",
        "# # EverythingLM-13b-V2-16K-GGUF\n",
        "# !wget -P models/ https://huggingface.co/TheBloke/EverythingLM-13b-V2-16K-GGUF/resolve/main/everythinglm-13b-v2-16k.Q6_K.gguf\n",
        "\n",
        "# # Chronohermes-Grad-L2-13B-GGUF (similar to PuddleJumper)\n",
        "# !wget -P models/ https://huggingface.co/TheBloke/Chronohermes-Grad-L2-13B-GGUF/resolve/main/chronohermes-grad-l2-13b.Q6_K.gguf\n",
        "\n",
        "# # Redmond-Puffin-13B-GGML\n",
        "# !wget -P models/ https://huggingface.co/TheBloke/Redmond-Puffin-13B-GGUF/resolve/main/redmond-puffin-13b.Q6_K.gguf\n",
        "\n",
        "# # Pygmalion-2-13B-GGUF\n",
        "# !wget -P models/ https://huggingface.co/TheBloke/Pygmalion-2-13B-GGUF/resolve/main/pygmalion-2-13b.Q6_K.gguf\n",
        "\n",
        "# # Mythalion-13B-GGUF\n",
        "# !wget -P models/ https://huggingface.co/TheBloke/Mythalion-13B-GGUF/resolve/main/mythalion-13b.Q6_K.gguf\n",
        "\n",
        "!wget -P models/ https://huggingface.co/TheBloke/llama2-22B-daydreamer-v2-GGUF/resolve/main/llama2-22b-daydreamer-v2.Q4_K_S.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozkIlCiSqiFu"
      },
      "source": [
        "Import the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8P8NOuvkB_i"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import PyMuPDFLoader, TextLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "import os\n",
        "import glob\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxnPdYdQqiFv"
      },
      "source": [
        "Mount Google Drive to Colab. This step is optional and is required only if you prefer to load the documents from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zc-gzJEW-GV",
        "outputId": "73824f93-d9aa-4b21-fa05-7929897c6033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCxxk6FOqiFv"
      },
      "source": [
        "Specify the paths of the documents directory (source_dir), the Chroma database (persist_directory), which will be created once the documents are loaded, and the quantized LLaMA model. (model_path)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAZ6ZrgVqiFw"
      },
      "outputs": [],
      "source": [
        "source_dir = \"drive/MyDrive/docs\"\n",
        "persist_directory = \"db\"\n",
        "model_path = \"/content/models/llama2-22b-daydreamer-v2.Q4_K_S.gguf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzEZjqH9qiFx"
      },
      "source": [
        "Load the documents from the specified directory and store them in a Chroma database. The supported file formats are .pdf and .txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcEWa6_nqiFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b11f8e0-5b46-48d9-e669-a4a601893686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00, 10.87it/s]\n"
          ]
        }
      ],
      "source": [
        "LOADER_MAPPING = {\n",
        "    \".pdf\": (PyMuPDFLoader, {}),\n",
        "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
        "}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def load_single_document(file_path: str) -> List[Document]:\n",
        "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
        "    if ext in LOADER_MAPPING:\n",
        "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
        "        loader = loader_class(file_path, **loader_args)\n",
        "        return loader.load()\n",
        "\n",
        "    raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
        "\n",
        "\n",
        "all_files = []\n",
        "for ext in LOADER_MAPPING:\n",
        "    all_files.extend(\n",
        "        glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n",
        "    )\n",
        "\n",
        "with Pool(processes=os.cpu_count()) as pool:\n",
        "    documents = []\n",
        "    with tqdm(\n",
        "        total=len(all_files), ncols=80\n",
        "    ) as pbar:\n",
        "        for i, docs in enumerate(\n",
        "            pool.imap_unordered(load_single_document, all_files)\n",
        "        ):\n",
        "            documents.extend(docs)\n",
        "            pbar.update()\n",
        "\n",
        "\n",
        "\n",
        "if not documents:\n",
        "    print(\"No documents to load\")\n",
        "else:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500, chunk_overlap=50\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    if os.path.exists(persist_directory):\n",
        "      shutil.rmtree(persist_directory)\n",
        "\n",
        "\n",
        "    db = Chroma.from_documents(\n",
        "        texts,\n",
        "        embeddings,\n",
        "        persist_directory=persist_directory,\n",
        "    )\n",
        "    db.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhtjotl3qiFx"
      },
      "source": [
        "Create a vectorstore from the created database and create an index for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK4cTSg-qiFy"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma(embedding_function=embeddings, persist_directory=persist_directory)\n",
        "index = VectorStoreIndexWrapper(vectorstore=vectorstore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkwpqAwyqiFy"
      },
      "source": [
        "Load the model with LlamaCpp. Please adjust the parameters n_gpu_layers and n_batch of the model based on you GPU specifications. The following values work well with the default GPU provided by Google Colab.\n",
        "\n",
        "**Note**: To disable token-wise streaming, comment out the **callback** parameter of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGmGR054yIRt"
      },
      "outputs": [],
      "source": [
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 64  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "llm = LlamaCpp(\n",
        "  model_path=model_path,\n",
        "  max_tokens=8000,\n",
        "  n_batch=n_batch,\n",
        "  n_gpu_layers=n_gpu_layers,\n",
        "  callbacks=[StreamingStdOutCallbackHandler()],\n",
        "  n_ctx=8000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a chain with Langchain. The parameter **chain_type** can be: \"stuff\", \"refine\", \"map_reduce\" or \"map_rerank\". For document-based question answering, the model produces better results with \"stuff\".\n",
        "\n",
        "For more information, please refer to the Langchain documentation: https://python.langchain.com/docs/modules/chains/document/"
      ],
      "metadata": {
        "id": "VYAdiCh_zjX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAoC1Oiz_I1Y"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "  llm=llm,\n",
        "  chain_type=\"stuff\",\n",
        "  retriever=vectorstore.as_retriever(),\n",
        "  return_source_documents=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM2_lQQQqiFz"
      },
      "source": [
        "Query the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldyjlWNck1QF"
      },
      "outputs": [],
      "source": [
        "question = \"What are some interesting facts about photovoltaic recycling?\"\n",
        "\n",
        "res = qa(question)\n",
        "answer, docs = (\n",
        "    res[\"result\"],\n",
        "    res[\"source_documents\"],\n",
        ")\n",
        "\n",
        "print(\"\\n\\n> Question:\")\n",
        "print(question)\n",
        "print(answer)\n",
        "\n",
        "for document in docs:\n",
        "    print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n",
        "    print(document.page_content)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}